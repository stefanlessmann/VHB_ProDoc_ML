{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a55ae35",
   "metadata": {},
   "source": [
    "<div style=\"position:relative; width:100%; height:200px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stefanlessmann/VHB_ProDoc_ML/master/banner-nb.png\" style=\"width:100%; object-fit:cover;\" alt=\"ProDok-MachineLearning-Banner\">\n",
    "  <div style=\"\n",
    "      position:absolute;\n",
    "      left:4%;\n",
    "      top:50%;\n",
    "      transform:translateY(-50%);\n",
    "      font-size:3.2vw;\n",
    "      font-weight:750;\n",
    "      color:#1f2a44;\">\n",
    "    ProDok â€“ Machine Learning\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42dabd9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stefanlessmann/VHB_ProDoc_ML/blob/master/P.I.2.cs_benchmark.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc7e176",
   "metadata": {},
   "source": [
    "# P.I.2 Machine Learning for Credit Risk Modeling: Model Selection, Evaluation, and Interpretation\n",
    "The practice sessions complement the lectures and provide hands-on experience with the concepts covered in the course.\n",
    "This session focuses on classic machine learning algorithms and practices. We will continue working with the credit risk analytics case, and benchmark alternative supervised learning algorithms to default prediction. In this scope, we will revisit data organization principles, classification model evaluation, and hyperparameter tuning. The notebook concludes with an outlook on the XAI (explainable AI lecture) by illustrating some standard XAI outputs to shed light on the patterns inferred by our learning algorithms. \n",
    "\n",
    "As in the previous practice session, the available time does not permit manual coding or extensive code reviews. We will provide coding demos for selected parts and otherwise rely on LLMs to generate the codes we need. The focus of the practice session is on prompt engineering and discussing ML outputs.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb647dcc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Data Preparation Reloaded\n",
    "\n",
    "**Context:** \n",
    "We received a new sample of credit risk data. The sample comprises the same features and was taken 6 month after the first 100k batch was gathered. We developed Python codes to ready the data for analysis. However, the code was AI generated and not designed for reusability. Our first task is to *refactor* the data preparation code to ensure can use for the first and the second batch of data, as well as future batches yet to come. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0699dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# We assume you stored the data locally. If not, use the URLs to load it from the web.\n",
    "#url_100k = \"https://raw.githubusercontent.com/stefanlessmann/VHB_ProDoc_ML/master/credit_data_100k.csv\"\n",
    "#url_25k = \"https://raw.githubusercontent.com/stefanlessmann/VHB_ProDoc_ML/master/credit_data_25k.csv\"\n",
    "\n",
    "df_100 = pd.read_csv(\"credit_data_100k.csv\")\n",
    "df_25 = pd.read_csv(\"credit_data_25k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d485e3",
   "metadata": {},
   "source": [
    "## Task: \n",
    "Use the code from `P.I.1.data_exploaration.ipynb` to create a reusable data preparation function. Apply your function to create *ready-for-modeling* versions of both datasets. Store the results in variables `dfDevelopment` and `dfHoldout`, respectively. \n",
    "\n",
    "In general, LLMs are good at refactoring code. Put our generated data preparation code into a prompt and instructing an LLM to refactor is should work just fine. One the other hand, all we need to do is wrapping up our previous code in a function. Decide freely if you want to approach the task by prompting an LLM or by coding the data preparation function yourself.\n",
    "\n",
    ">Note that this exercise focuses on classification. Therefore, make sure to remove the `LGD` column from the data frames to prevent data leakage.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8377f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place for your data preparation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10071714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place to call your function and creating modeling-ready datasets\n",
    "dfDevelopment = None # Put your solution here\n",
    "dfHoldout = None # Put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f76aad",
   "metadata": {},
   "source": [
    "# Overfitting demonstrated\n",
    "The lecture introduced you to the *fundamental problem of overfitting*, claiming that decision trees are particularly vulnerable to overfitting to make a case for tree pruning. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/Humboldt-WI/demopy/main/tree_pruning.png\" alt=\"Overfitting in decision trees\" width=\"480\">\n",
    "</p>\n",
    "\n",
    "## Task:\n",
    "\n",
    "Verify the illustrated relationship between tree depth and overfitting. To that end, your task is to:\n",
    "1. Partition the data of `dfDevelopment` into a training and validation set using ratios of 80%/20%.\n",
    "2. Train a decision tree classifier on the training data with varying tree depth (e.g., from 1 to 10).\n",
    "3. Evaluate the performance of each model on both the training and validation sets using an appropriate metric.\n",
    "4. Plot the training and validation performance against tree depth to visualize the overfitting phenomenon.\n",
    "\n",
    "The coding tasks asks for a basic ML pipeline involving functions/classes `train_test_split`, `DecisionTreeClassifier`, amongst others. It is a good exercise to code the solution yourself, but you can also use AI; a lazy prompt should do, no discussion of a prompting strategy warranted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83642c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place for your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e89d09",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "We finally hit the main part of the practice session: benchmarking different machine learning algorithms for credit risk modeling. This exercises addresses a common questions in machine learning applications: *which algorithm is best suited for my data?*\n",
    "Research papers that introduce novel methodologies also include benchmarking exercises to demonstrate the superiority of their method. Lastly, a learning goal of this exercise is to familiarize you with standard machine learning workflows and relevant Python functions/classes. \n",
    "\n",
    "We frame the benchmarking task such that it is realistic and perhaps a bit challenging. We discuss details in class, including our prompting strategy for the coding part. \n",
    "\n",
    "## Task: \n",
    "Compare several established machine learning algorithms using the development sample (i.e., `dfDevelopment`). Consider logistic regression (LR), neural networks (NN), decision trees (DT), random forests (RF), and extreme gradient boosting (XGB). For algorithms that exhibit hyperparameters, make sure these are properly tuned. Assess the performance of the different algorithms in terms of i) the area under the ROC curve, ii) the area under the prediction-recall curve, iii) the F-score, and iv) the Brier score (i.e., the mean squared error of the predicted probabilities compared to a zero-one coded target). Produce an estimate of model performance for each algorithm on a suitably selected subset of the development data. Also assess models on the holdout data (i.e., `dfHoldout`). Plot the ROC-curve, the PR-curve, and the confusion matrix for logistic regression and the overall best-performing model (if different) on the holdout sample.  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8be9816e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CODE GENERATION PROMPT  - to be completed in class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place for your benchmarking code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
