{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProDok-ML - PreCourse - Task 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QFRhSDwwlUln",
        "hLTCVrDqRCeh"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanlessmann/VHB_ProDoc_ML/blob/master/ProDok_ML_PreCourse_Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r3pvDQF9hGk",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Distinguishing borrowers into good and bad credit risks\n",
        "In our second task, we continue working on binary classification using  logistic regression. However, instead of synthetic toy data, we will use a real world-data set referring to risk management and decision-making in credit scoring. Compared to our first task, this classification problem is much harder to solve and requires a number of steps to get a good result. The following tasks will guide you through the process of loading, integrating, and preparing the data to eventually develop a logistic regression model. The final model predicts the probability of a credit applicant to default, meaning that the outstanding amount would not be paid back (in full). If you would like some  background information on retail credit risk assessment, have a look at our paper on[ benchmarking classification models for credit scoring](https://doi.org/10.1016/j.ejor.2015.05.030).\n",
        "\n",
        "Should you feel that logistic regression is a rather simple model for a machine learning course, I can assure you that we will look at several more advanced learning algorithms in the course and apply some of these to the same credit scoring data you are about to use. That said, note that logistic regression is the single most important model for loan approval decisions in the industry. Familiarity with logistic regression is crucial for every data scientists and  machine learners. \n",
        "\n",
        "Also note that the focus of the task is not on developing a highly accurate classifier. This will soon become our goal but first we need to do some homework: **data preparation**. Data preparation is the core of the task. The data set is challenging and you need to work hard to bring the data into a form that facilitates building classification models. In the scope of doing so, you will work intensively with Python libraries commonly used in machine learning such as Pandas. Becoming acquainted with these libraries and gaining experience in working with complex, messy real-world data is the core of the task. \n",
        "\n",
        "So let's being with loading some of these standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjjQ9GwyAGzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading some libraries we will need soon\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxFmZc5jO18l",
        "colab_type": "text"
      },
      "source": [
        "We are ready to start working with the data that we obtained from Kaggle. As has probably become clear already, the following tasks are nontrivial and might challenge you quite a bit. Work hard to solve them. It is definitly possible. Since the data is wel-known, you can also find much information and tips online, e.g., in the forum of the [Kaggle competition](https://www.kaggle.com/discussion). Shoud you nonetheless struggle with getting the data ready for analysis, using a shortcut is better than giving up. Much of the complexity of the following tasks comes from us integrating different sources of data. Getting the data ready for analysis would be a lot easier when using only the single data file *application_train.csv*. Using only this file, you could still perform many of the tasks we discuss below including handling missing values and outliers, scaling numeric variables, encoding categories, and, of course, developing a logistic regression classifier. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1NCFGhGjm-7",
        "colab_type": "text"
      },
      "source": [
        "## Create the initial dataset\n",
        "We will consider a subset of the available data from the [Kaggle Home credit default risk competition](https://www.kaggle.com/c/home-credit-default-risk/data):\n",
        "- application_train\n",
        "- application_test\n",
        "- bureau\n",
        "- previous_application\n",
        "\n",
        "The terms *training* and *test* data are omnipresent in (supervised) machine learning. We use the training data to build a model and the test data to assess that model. Importantly, the assessment involves applying the fully-specified (i.e., train**ed**) model to data that was not used before during model development (i.e., train**ing**). Another, more technical, important difference between the train and test data is that the former provides true labels for observations (i.e., a binary default / no-default indicator) whereas the latter does not. By observations we mean the rows of a DATA table, which correspond to bank clients who apply for a loan in the home credit data set.\n",
        "\n",
        "First of all, we are required to load the pre-processed h5 file as well as the datasets it contains. This requires you to have executed the data integration notebook for the home credit data and that the h5 file is available. Should that not be the case make sure to run the data integration notebook before moving on.\n",
        "\n",
        "\n",
        "We will now merge both train and test data with the other two datasets. First load the four above data sets into different pandas data frames using the *read_hdf* function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4h-DluazsUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here is your first job: \n",
        "# Load the following data sets from the file data.h5, which you created when\n",
        "# executing the data integration notebook. Create one dataframe for each data \n",
        "# set. \n",
        "#   application_train (appl_train)\n",
        "#   application_test (appl_test)\n",
        "#   bureau (bureau)\n",
        "#   previous_application (prev)\n",
        "# where the entries in brackets suggests names for the corresponding variables.\n",
        "appl_train = \n",
        "appl_test  = \n",
        "bureau     = \n",
        "prev       = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "819Zp4SdyYEE",
        "colab_type": "text"
      },
      "source": [
        "Well done!\n",
        "\n",
        "I know that the task to load some data seems rather trivial. However, especially when working with colab, some seemingly rudimentary tasks are not so easy and probably new to participants who have not used cloud-based services before. Hence, good you made it till here!\n",
        "\n",
        "You may have wondered why we have to bother with four different files at all. Well, companies routinely store data in databases and these consist of individual tables. The organization of the data at Kaggle mimics this approach. Different pieces of information are available in different tables, which can be linked to one another via shared columns (called foreign keys in database jargon). \n",
        "\n",
        "### Integrating the data\n",
        "Our next task is to merge the data set to create one big table. To that end, you first have to identify suitable key columns, which facilitate joining the different data sets. Note that the data set [homepage](https://www.kaggle.com/c/home-credit-default-risk/data) offers a nice graph of the structure of the data. Considering that graph, identifying suitable keys should not be that difficult. Once accomplished, the missing bit is a suitable function to merge data frames. A good time to ask your favorite search engine for help ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGwOSoWSqJ5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merge the data on previous applications (prev) into the training data (appl_train)\n",
        "# and also into the test data (appl_test)\n",
        "train = \n",
        "test  = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYMDqH39S1rN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next merge the credit bureau data into the training and test set.\n",
        "train = \n",
        "test  = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNoPYScr_qLA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Note that the above operation of merging the data is equivalent to a join operation in SQL. Did you now that SQL knows different ways to join tables? Check which type of join you have performed, left, right, or inner. And then ask yourself what type would the correct for our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8_IAQUO2Ub",
        "colab_type": "text"
      },
      "source": [
        "At this point, you should have two dataframes that store the training and test data, respectively. Run the following code cell to get an overview of all variables in your workspace. Also add code to display the dimensionality of your data, that is the number of rows and columns. Do this for the training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMLEKsXiADjA",
        "colab_type": "code",
        "outputId": "2ac18f45-ee78-4995-b0a8-95c1e064a730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "# Run this to get an overview of variables in your workspace\n",
        "%whos\n",
        "\n",
        "# Write some code to query the dimensionality of your dataframes. \n",
        "# Put differently, how many rows and columns are available in the \n",
        "# training and test set?\n",
        "print('\\nDimension of the training and test set:')\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable     Type         Data/Info\n",
            "-----------------------------------\n",
            "appl_test    DataFrame           SK_ID_CURR  ...  a<...>[48744 rows x 26 columns]\n",
            "appl_train   DataFrame            SK_ID_CURR  ...  <...>307511 rows x 27 columns]\n",
            "bureau       DataFrame             SK_ID_CURR  ... <...>716428 rows x 12 columns]\n",
            "drive        module       <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "gc           module       <module 'gc' (built-in)>\n",
            "np           module       <module 'numpy' from '/us<...>kages/numpy/__init__.py'>\n",
            "pd           module       <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "plt          module       <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
            "prev         DataFrame             SK_ID_CURR  ... <...>670214 rows x 30 columns]\n",
            "test         DataFrame             SK_ID_CURR  ... <...>582679 rows x 66 columns]\n",
            "train        DataFrame             SK_ID_CURR  TARG<...>091522 rows x 67 columns]\n",
            "Dimension of the training and test set:\n",
            "(8091522, 67)\n",
            "(1582679, 66)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTc__9tHPnJs",
        "colab_type": "text"
      },
      "source": [
        "You should see a bit more than 8 mio. rows in the training set and about close to 1.6 mio. rows in the test set. That is a lot of data! Complete the following code cell to query the dimensionality of the original dataframes that you have loaded from data.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQAmeStyQavY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Query dimensionality of the four dataframes \n",
        "# appl_train, appl_test, bureau, and prev.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GngHZRkKRAXv",
        "colab_type": "text"
      },
      "source": [
        "Hopefully the above task was easy for you. It actually produced an important result. See how the original data was much smaller. For example, you should see 307511 observations in *application_train*. However, the number of observations in the dataframe *train*, which you created by merging the application, bureau, and previous applications data, was much larger; i.e., about 8 mio. Why is that?\n",
        "\n",
        "### Understanding data integration\n",
        "The answer is related to the way in which dataframes (tables) are merged (joined). I encourage you to spend some time on this matter to understand how we ended up with more than 8 mio. rows in the training data. Eyballing the data using the Pandas function *head() is a good starting point.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CCPwAefSYUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the function head() to get on overview of the training data after merging\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm8nOKWWSvjR",
        "colab_type": "text"
      },
      "source": [
        "You should have seen a pattern that one credit applications, with a given ID, re-appears. Noting this pattern is the starting point to explaining the difference between 307511 credit applications in the training set (indeed, we do only have that many applications) and the more than 8 mio. observations we obtain after merging. Merging reproduces rows. \n",
        "\n",
        "Imagine a single credit applicant. Let's call her Eve. Eve applies for a loan. We would see one entry for Eve in the original training data (application_train at the Kaggle website). The data file previous_applications provides information about the credit history of Eve. Let's assume she had applied for ten different loans in the past. So the previous_applications file has ten entries for Eve. Merging application_train and previous_applications, we obtain ten entries for Eve in the resulting dataframe. The values for the variables (i.e., columns) of the application data re-occur over these ten rows, whereas the values in the columns belonging to the previous application data will vary. In the above sketch of the data, you can scroll to the right to verify that such a pattern exist. A first block of colums displays re-occuring values. Later on, there is a block of columns the values of which do not re-occur.\n",
        "\n",
        "I recommend you go back to previous steps, re-run cells, maybe playing a bit with the *head()* function, etc. to make sure you fully understand what has happend with the data before moving on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deRxDMmorw9L",
        "colab_type": "text"
      },
      "source": [
        "### Moving on\n",
        "Maybe it is time to clean up a little bit. Since you have the training and test data integrated, you do not need the dataframes that you created when loading the data from disk. Delete these and other unneeded variables from memory. This is important to avoid your notebook chrashing because of out of memory errors. Remeber, the training and test set have, togehter, close to 10 mio. observations. We better pay attention to how much memory or program consumes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epwz_-WWsH73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del bureau, prev, appl_train, appl_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keNWWScWqguS",
        "colab_type": "text"
      },
      "source": [
        "Machine learning commonly involves processing *large* data sets. The previous steps gave us a feeling for such data. Subsequent steps will also exemplify some of the specific challanges that arise when working with larger data sets. What I mean by that? You should ready yourself for experiecing your notebook to crash due to out of memory errors and the like. Your will also notice that certain computations take a 'little' time to complete. Do not allow such issues to frustrate you should they occur. Using machine learning in your research will involve processing nontrivial data sets. So it is good to get familiar with somewhat complex data and the home credit data was deliberately chosen for that reason.\n",
        "\n",
        "One idea to keep the level of frustration as low as possible. Whenever you have accomplised a task and created a new version of the data, you can store it to an h5 file. The data integration notebook has familiarized you with that operation. The benefit of storing intermediate results is that when your code crashes at some point, you can load the intermediate result (i.e., data) and re-start from there. This is easier compared to having to re-start from scratch every time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFRhSDwwlUln",
        "colab_type": "text"
      },
      "source": [
        "## Explorative data analysis (EDA)\n",
        "\n",
        "Finally, we are ready to start working with the data. As a rule of thumb, the first step is always to inspect the data and get a first impression.\n",
        "\n",
        "Your job is to figure out which of the variables in the data set, in other words columns, might be important for classifying observations (aka rows, aka credit applications) into good and bad risks.  \n",
        "\n",
        "In exploratory data analysis (EDA), as the name entails, we explore data - in particular by plotting the distribution of different variables or by looking at the correlation between variables.\n",
        "\n",
        "First of all, take a look at the distribution of the target variable.\n",
        "\n",
        "Then, take three variables from all three sources (application, bureau, previous_application). Create variable distribution and correlation plots to check whether the selected variables appear important. The packages matplotlib pyplot (already loaded) or alternatively seaborn will help you here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAF24SV47naI",
        "colab_type": "text"
      },
      "source": [
        "### Application:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA8CG1K6rUNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the distribution of the target variable\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI190ivcG719",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start out with a correlation plot -> strong correlation with the target variable \n",
        "# might be an indication that a variable is important. However, since there are \n",
        "# many variables in the data, I recommend you restrict the plot to a subset.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4xRGlRmuvsX",
        "colab_type": "code",
        "outputId": "e9fd401a-5770-4ced-d1fa-2cd8bfe35e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Comparison is difficult when not using density plots / stratified plots considering the target variable.\n",
        "# Out of the set of variables you consider above, select the one which is most \n",
        "# correlated with the target and visualize the variable's distribution for \n",
        "# good and bad credit applicants\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD4CAYAAADLhBA1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlcplN7iDWM8",
        "colab_type": "text"
      },
      "source": [
        "Play around by running your code for other variables and examine their distribution across the two states of our target variable, good or bad, which are encoded as zero and one, respectively in the data. Make sure to also plot the distributions using both counts and relative frequencies.\n",
        "Hint: check out the option *density=True*.\n",
        "Finally, recall that we have previously merged different sources of data, application data, which consists of the information a credit applicant provides upon applying for a loan, bureau data, which is gathered by credit bureaus like the Schufa in Germany of Experian in the US, and data from previous credit applications of the same applicant. Make sure to examine variables (create plots) from each of these sources, and remember your focus: you strive to identify *important* variables. This is why your plots should display a variable's distributions for good and bad customers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eaxbbu_sFCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for exploring other variables goes here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITWx_XyxtLS",
        "colab_type": "text"
      },
      "source": [
        "Ok, that should be enough plotting for now. Did you produce any meaningful findings? Any clues which variables matter the most and which source of data is most relevant? Any evidence of outliers, data errors or the like? These are exemples of the type of questions EDA is meant to answer.\n",
        "\n",
        "## Data preparation\n",
        "\n",
        "Now that you have created the dataset and have a good understanding of it, it is time to prepare the data for the binary classification task. Likely, while trying to find relevant variables, you already noticed some issues with the data. In this sub-task, we will deal with those issues. First of all, we store the target variable of the train dataset in a separate dataframe and delete it from the train dataset. This allows us to combine the train and test dataframes, which, in turn, is useful because we want to apply operations such as imputing missing values or scaling the values of a variable to the training and the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRudopX9-A2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the target variable from the training data and save it for later in a seperate dataframe\n",
        "# Next, remove the target variable from the dataframe storing the training data\n",
        "# Finally, create one large dataframe that stores both the training and test data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNPcAoIyGY6a",
        "colab_type": "text"
      },
      "source": [
        "By the way, before moving on, are you sure you solved the previous task? \n",
        "\n",
        "For those credit applicants that were part of the training set, you extracted the value of the target variable. Check. \n",
        "\n",
        "Then you added the data from the applicants that formed the test data to the training data. Check. \n",
        "\n",
        "So now you have one combined dataframe with all information on credit applicants (i.e., their variables) and another dataframe or vector that stores the target variable. \n",
        "\n",
        "Ok, sounds good. But to make sure your code is ready for subsequent tasks, check whether your solution would, in theory, allow you to revert these steps. You certainly remember our friend Eve. Eve was an applicant who appeared in the training data. We do know the true value of the target variable for Eve. After extracting the target variable from the training dataframe, would you still be able to tell the value of the target variable of Eve? And would you also be able to do so if we, for some reason, randomly permute the rows in the dataframe? \n",
        "\n",
        "If yes all is well. You can move on even if unsure but note that you may have to come back to the previous operation later and revisit the way in which you extracted the target variable from the training set.\n",
        "\n",
        "One other point before moving on. How many observations (rows) do you have in the dataframe storing the target variable? We have 307511 credit applications in the training data so your answer should be 307511, right? I bet you get a different result when checking. What has happend and how can this be overcome? How to make sure that you have exactly 307511 values of the target varible? And are able to connect these values to the original credit applications? (Machine learners') Life is hard at times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M770NGkyJYu",
        "colab_type": "text"
      },
      "source": [
        "### Time / Date based variables\n",
        "\n",
        "Some variables in the data set represent time information, such as the age of a credit applicant (app_DAYS_BIRTH). Examine the way in which this information is stored. You could, for example, consider a histogram or simply check the value range of the variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcVkaVlEiGs4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Depict the distribution of the variable 'app_DAYS_BIRTH'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMGnWMEZiG3T",
        "colab_type": "text"
      },
      "source": [
        "You will have noticed that all entries for the variable are negative. It is common practice to store time-related information in such a way, e.g., the number of days that have past since some reference date. However, for data analysis, this format is less suitable. Your next task is to convert time information into a more useful format. This should be done for each variable that stores time related information. However, for simplicity we will only do the following:\n",
        "1. Identify variables that store time information\n",
        "1. Convert the values stored in those variables to positive numbers\n",
        "1. Calculate the age of an applicant in years and store it in a new variable *app_AGE_YEARS* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JL-yBtny9-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solutions to the three steps:\n",
        "# 1. Determine relevant variables\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGAT9G8bzKfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Convert all days_vars to positive numbers\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLPsqX_E2SM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Calculate age of applicant (ignore leap years for simnplicity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTkj1kp808Vc",
        "colab_type": "text"
      },
      "source": [
        "### Numerical variables\n",
        "Information is often stored as a numerical variable. The variables you processed above are an example. The age of an applicant is a numerical variable. Likewise, a credit score, an applicant's income, the amount of a loan, etc. are numerical variables.\n",
        "\n",
        "Often, the scale of numerical variables varies substantially. A variable may also exhibit extreme values (i.e., outliers), which can cause problems. In this sub-task, you have to make choices concerning handling potential outliers.\n",
        "Try to find all numerical variables and apply outlier detection. Box-Plots are always a helpful tool to check for outliers, however there might be other (more automatic) strategies to handle outliers.\n",
        "\n",
        "The definition what value constitutes an outlier is somewhat subjective. Make a sensible decision and replace outliers by a missing-value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9s-gFAH30l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detect outliers within numeric variables and replace them with a missing value\n",
        "\n",
        "\n",
        "# calculate z-score for every col, remove values greater 2 standard deviations -> this can also be done with scipy.stats zscore\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FZeh1eBenBM3"
      },
      "source": [
        "#### Memory Management\n",
        "\n",
        "With larger datasets, it is common to run into memory issues. Recent versions of Pandas (i.e., Pandas 1.x) are maybe are bit more robust but in general, it is good practice to pay attention to memory consumption. An easy way to free up memory is to convert binary variables, also called FLAG variables, to a small integer type and to reduce the float size of the other numeric variables. \n",
        "\n",
        "Make sure you fully understand the following code. Note how it helps with reducing the amount of consumed memory. If the code looks somewhat familiar you are correct. The data integration notebook uses a similar approach. Well spotted.\n",
        "\n",
        "Should the code produce an error, this is likely because you use different variable names than I use in my solution. For example, I name the dateframe with the integrated data *data*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6wZOM9AUnBMn",
        "outputId": "0400c247-7e92-465d-e015-7a1a7a2ba076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Print the amount of memory your notebook is currently using\n",
        "print('Memory use before cleaning up:')\n",
        "print(str(data.memory_usage(index=True, deep=True).sum()//1024//1024)+' MB')\n",
        "\n",
        "floats = [x for x in data.columns if data[x].dtype in ['float16', 'float32', 'float64']]\n",
        "data[floats] = data[floats].astype('float32')\n",
        "flags = [x for x in data.columns if 'FLAG' in x]\n",
        "\n",
        "data[flags] = data[flags].astype('int8')\n",
        "\n",
        "print('Memory use after  cleaning up:')\n",
        "print(str(data.memory_usage(index=True, deep=True).sum()//1024//1024)+' MB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory use before cleaning up:\n",
            "3642 MB\n",
            "Memory use after  cleaning up:\n",
            "2941 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S-oMpXc4AJu",
        "colab_type": "text"
      },
      "source": [
        "### Categorical variables\n",
        "\n",
        "Another important type of variables are categorical variables. These are variables that  do not have an explicit numeric meaning but represent different categories or classes. An example for a categorical variable could be the occupation of a person (e.g. Student, Factory-worker, Accountant...). Each category level provides information about a person but you cannot transfer the different occupations into a numeric space. Some algorithm implementations have built-in functions to deal with categories. Typically, however, this is your job. In this sub-task, we will use one-hot encoding (also referenced as dummy encoding) where every class becomes a binary variable. For more information read e.g. [this post](https://towardsdatascience.com/one-hot-encoding-multicollinearity-and-the-dummy-variable-trap-b5840be3c41a).\n",
        "\n",
        "Before performing the actual coding operations, however, we need to attend to one issue in our data. You have probably not noticed it but three variables have changed their data type from category to object when we combined the training and test data. Feel free to diagnose why his happend if you are interested. It has to do with different category levels in the training and test set. Without going into details, we want to convert the affected variables back to data type categorical. So determine which variables are stored as data type object, verify that these variables are indeed categorical variabels and convert them to data type category before moving on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvMl7G8fwLCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Identify variables of data type object and convert these to data type category\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0cr78TxweGj",
        "colab_type": "text"
      },
      "source": [
        "At this point, your data set should include 24 categorical variables. Verify this is the case, Then transform the categorical variables using dummy coding. There is a nice function to do so with just one line of code. \n",
        "\n",
        "Make sure to apply reference coding. For example, if you have a categorical variable with three levels: red, green, blue you need only 3-1=2 two dummy variables to encode the information of the category as, e.g., isRed {0,1}, isGreen {0,1}. If both dummies are equal to zero, this implies that the category level of the observation before dummy coding was blue. In other words, you do not need a third dummy variable isBlue {0,1} to encode the category. Using n-1 dummy variables, with n denoting the number of category levels, is called reference coding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYmTqyeaKt_1",
        "colab_type": "code",
        "outputId": "f63e4abf-6db0-4929-f10a-5191f6cfa4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Do you have 24 categorical variables? Check. \n",
        "\n",
        "# Also print the total number of columns in your data\n",
        "\n",
        "# Encode categories as dummy variables and check again how many columns you have in your data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of categories is: 24\n",
            "Number of columns is: 67\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9674201, 274)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7oXk4d30ECx",
        "colab_type": "text"
      },
      "source": [
        "Did you notice the sharp increase in dimensionality? This is the disadvantage of dummy coding. In practice, we would need to process categories in a more elaborate way to avoid creating so many new dummy variables. If you are interested, have a look at [this paper](http://dx.doi.org/10.1016/j.dss.2015.02.007) to learn about alternative, better options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtLljOzsS5EL",
        "colab_type": "text"
      },
      "source": [
        "### Missing Values\n",
        "\n",
        "Another common issue in real world data are missing values. For different reasons, data may be incomplete. Many state-of-the-art implementations of machine learning algorithms have built-in algorithms to deal with missing values. However, this task will have you deal with them yourself.\n",
        "\n",
        "The two major paths to take here are either:\n",
        "- To drop the rows and/or columns with missing values\n",
        "- To impute the missing values\n",
        "\n",
        "To get an idea on the different possible approaches, read e.g.: [this](https://medium.com/ibm-data-science-experience/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87)\n",
        "\n",
        "Scikit learn offers a good imputation algorithm that might also be helpful: [Sklearn Imputation](https://scikit-learn.org/stable/modules/impute.html). \n",
        "\n",
        "Try to deal with all missing values in the dataset applying the strategies of your choice.\n",
        "\n",
        "**Warning:** The data set is large, as is common practice in machine learning. You will most likely experience problems with memory errors and need to think carefully how you can perform calculations in an efficient manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HDq_InfUNVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write code to replace missing values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fcDXeAuCXoM",
        "colab_type": "text"
      },
      "source": [
        "The above steps have probably taken you some time. To make sure that you do not need to repeat them if something goes wrong in subsequent tasks (e.g., crashing your notebook), you should save your data at this point. The below code illustrates how but note that you might have to adjust the code because of you using different names for variables, different folder paths, etc. In the demo code, the relevant variables are called *data* and *y* and they store the data on credit applicants and target variables, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2el9YzHq87Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save data to disk\n",
        "data.to_hdf('data_pp.h5', key='data', mode='w')\n",
        "y.to_hdf('data_pp.h5', key='y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ1115w4C4jY",
        "colab_type": "text"
      },
      "source": [
        "So from here onwards, we could load the preprocessed data and continue working with it. If you use colab, you should also download the h5 file created above to store it permanently on your hard disk or a cloud drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNC1eJJh7SfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can use this point to re-load the data if somethings goes wrong.\n",
        "# Simply un-comment the below lines if you actually want to load data\n",
        "# but note that the code needs adjustment to match your folder path and\n",
        "# variable names\n",
        "\n",
        "#y = pd.read_hdf('data_pp.h5', key='y')\n",
        "#data = pd.read_hdf('data_pp.h5', key='data')\n",
        "#!cp 'data_pp.h5' '/content/drive/My Drive/data_pp.h5' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExPrbcUsU6qS",
        "colab_type": "text"
      },
      "source": [
        "### Feature Engineering\n",
        "\n",
        "Algorithms can detect many patterns in data. However, algorithms lack domain knowledge. Feature engineering is the most time consuming and one of the most important tasks in machine learning. It is all about transforming variables in such a way that finding relevant patters in data becomes easier for the algorithm. \n",
        "\n",
        "There are a number of basic transformation to enhance the informational value of a variable including, for example: \n",
        "- Logarithm\n",
        "- Multiplications / Ratios\n",
        "- Aggregations (Min, Max, Mode, Mean, Unique count, Sum, ...) \n",
        "\n",
        "The 'holy grail', however, is to create new features based on domain knowledge. This could involve, for example, combining multiple variables into a single variable by calculating ratios, moving averages, etc. In this task, you should try to create variables representing the following information:\n",
        "\n",
        "- A credit/income ratio\n",
        "- An annuity/income ratio\n",
        "- How much of their life a person has worked\n",
        "- How long the credit will run for until its paid off (Is this correct??)\n",
        "- A ratio on how much credit was given compared to for what a person applied for\n",
        "- Number of previous applications\n",
        "- Previous acceptance ratio\n",
        "- Number of Active, Bad and Closed loans \n",
        "\n",
        "Later on, you can try to come up with yet more interesting or powerful variables to improve your analytical model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol6STmfXWISM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write code to create the above features. \n",
        "# Feel free to also engineer additional features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMC8s_FZ2LMm",
        "colab_type": "text"
      },
      "source": [
        "## Aggregating the data\n",
        "Finally, we come back to the point that our data does not yet has one row per applicant but many rows per applicant. We have elaborated on this issue above. Go back to the part on *Understanding data integration* if feeling a little shaky. \n",
        "\n",
        "The goal of our next task is to obtain a dataframe with exactly that many rows as there are in the original training set (application_training) and original test set (application_test). Remember that the other two data sets concerned with previous credit applications and burea data have rows and that this has amplified the number of observations when merging the data. \n",
        "\n",
        "So your task is to aggregate the data in a 'suitable' way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0p_gKy2Ya8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aggregate the data in a suitable way. In the end, you should have a dataframe\n",
        "# with exactly that many rows as there are in the original data files \n",
        "# application_train and application_test together. Write code to verify this.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8PTV8J7jMq",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing numeric features\n",
        "\n",
        "Normalization is applied such that high variable values do not have a higher impact just because their value is high. For example, if we were to include both income and number of children, income will be in the thousands and number of children likely in the one-digits.\n",
        "\n",
        "We held of normalizing features, as we hadn't yet used them to create new features. Now normalize all features by substracting the mean and dividing by the standard deviation. Run a search for *z-score* for some background information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pojR66c3tzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize variable values by calculating z-scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-NaJOKmWXIG",
        "colab_type": "text"
      },
      "source": [
        "### Finalizing the preprocessing\n",
        "\n",
        "As a final step, you will have to split the data again into the original train and test set. Splitting the prepared data will allow us to train a model on the training and then apply the model to the test set. To perform the splitting, however,  it will be important that you have stored and maintained relevant ID variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFqZXVQSpWlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write code to seperate your data into three dataframes.\n",
        "# Eventually, you should have one dataframe containing the cases of the \n",
        "# original training set (application_train), one dataframe containing the\n",
        "# cases of the original test set (application_tests) and one more dataframe\n",
        "# storing the target variable of training set cases. We created the latter\n",
        "# dataframe in a previous task and it should still be available in your \n",
        "# memory.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaCfRFEs4gyK",
        "colab_type": "code",
        "outputId": "a71884c1-6735-4305-daf7-8f51662e97a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Verify that the shapes of your splitted data match original data \n",
        "print(\"Training dataset shape: \",train.shape) # should have 307511 rows\n",
        "print(\"Test dataset shape: \",test.shape) # should have 48744 rows"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset shape:  (307511, 475)\n",
            "Test dataset shape:  (48744, 475)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu6q4b1y45xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save final preprocessed data to easily reload for modeling\n",
        "train.to_hdf('data_fin.h5', key='train', mode='w')\n",
        "test.to_hdf('data_fin.h5', key='test')\n",
        "label.to_hdf('data_fin.h5', key='label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzL8kKIO5U_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure to permanently store the data if you use colab\n",
        "!cp 'data_fin.h5' '/content/drive/My Drive/ProDok/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5RPpMkC5R2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And reload if necessary:\n",
        "train = pd.read_hdf('/content/drive/My Drive/ProDok/data_fin.h5', key='train')\n",
        "test = pd.read_hdf('/content/drive/My Drive/ProDok/data_fin.h5', key='test')\n",
        "label = pd.read_hdf('/content/drive/My Drive/ProDok/data_fin.h5', key='label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjz4ReMUWREo",
        "colab_type": "text"
      },
      "source": [
        "## Creating the model\n",
        "Now that you successfully prepared the dataset, it is time to create a model for the task. We will return to the logistic regression classifier, which you could experience in the first task of the pre-course assingment.\n",
        "\n",
        "Given that the problem we are trying to solve is a binary classification problem, logistic regression is a good fit.\n",
        "\n",
        "Train a logistic regression on the train part of your dataset. This might fail. In a subsequent step we will evaluate the model and try to come up with a better one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eEs12cl7PX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write code to estimate a logistic regression model. \n",
        "# In task 1, you have used the statsmodel library and scikit-learn. I recommend\n",
        "# you now use scikit-learn because the data is large and the scikit-learn \n",
        "# implementation of logistic regression is more efficient. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8AAfPu1Heqy",
        "colab_type": "text"
      },
      "source": [
        "### Attempting to to find the issue with the regression model:\n",
        "So while everything worked fine in the first task, the logit model we train on this data set does not converge. Any ideas why?\n",
        "\n",
        "Multi-collinearity might be a problem because we created such a large data. Try to identify variables that are highly correlated. You can examine different thresholds for *high*. Highly correlated variables should be removed. Do so and try again a estimate the logit model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU3B9zgVIaxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create correlation matrix and identify highly correlated variables\n",
        "# Afterwards, remove one variable of each pair of correlated\n",
        "# variables from the data. Check how many variables you have managed to remove \n",
        "# in this way. You need rather low correlation thresholds to achieve a sizeable\n",
        "# reduction of dimensionality.\n",
        "# Try re-estimating the logit model after removing correlated variables. Note \n",
        "# that will not succeed even if you set the correlation threshold as low as 0.5. \n",
        "# Nonetheless, solving the task to remove correlated variables is a useful \n",
        "# programming exercise. \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcthW3kkRVFa",
        "colab_type": "text"
      },
      "source": [
        "### Using a pre-selected variable subset\n",
        "We do not want to conclude the exercise without having estimated at least one logit model. Therefore, we pre-defined a set of variables. Remove all other variables from the data and estimate a logit model that includes the following variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jxH8FpfTAAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Estimate a logit model with these variables if you have successfully \n",
        "# integrated the different data files including bureau data and previous applications \n",
        "var_selection=['app_EXT_SOURCE_MEAN',  'app_EXT_SOURCE_2',  'app_EXT_SOURCE_3',  'bur_DAYS_CREDIT_mean',  'pre_CODE_REJECT_REASON_SCOFR_sum',\n",
        "               'bur_DAYS_CREDIT_max',  'bur_DAYS_CREDIT_UPDATE_mean',  'app_AGE_YEARS',  'pre_NAME_CONTRACT_STATUS_Refused_mean',\n",
        "               'app_EXT_SOURCE_1',  'pre_NAME_PRODUCT_TYPE_walk-in_sum',  'app_CODE_GENDER_M',  'app_NAME_INCOME_TYPE_Working',\n",
        "               'app_NAME_EDUCATION_TYPE_Higher education',  'pre_CODE_REJECT_REASON_SCOFR_mean',  'pre_ACCEPTANCE_RATIO',\n",
        "               'bur_DAYS_CREDIT_UPDATE_max',  'pre_NAME_PRODUCT_TYPE_walk-in_mean',  'pre_NAME_CONTRACT_STATUS_Refused_sum',\n",
        "               'bur_DAYS_CREDIT_min',  'app_REGION_RATING_CLIENT_W_CITY', 'app_PERCENT_WORKED',  'bur_DAYS_CREDIT_ENDDATE_min', \n",
        "               'app_ANNUITY_LENGTH',  'pre_CODE_REJECT_REASON_XAP_mean']\n",
        "\n",
        "# If you took the shortcut and used only the application data, you can use these\n",
        "# variables for your final logit model\n",
        "app_only_vars=['app_EXT_SOURCE_2',  'app_EXT_SOURCE_3',  'app_DAYS_BIRTH',  'app_EXT_SOURCE_1', \n",
        "                 'app_CODE_GENDER_M',  'app_NAME_INCOME_TYPE_Working',  'app_NAME_EDUCATION_TYPE_Higher education',\n",
        "                 'app_REGION_RATING_CLIENT_W_CITY',  'app_DAYS_EMPLOYED',  'app_REGION_RATING_CLIENT', \n",
        "                 'app_NAME_EDUCATION_TYPE_Secondary / secondary special',  'app_DAYS_LAST_PHONE_CHANGE', \n",
        "                 'app_REG_CITY_NOT_WORK_CITY',  'app_FLAG_EMP_PHONE',  'app_NAME_INCOME_TYPE_Pensioner', \n",
        "                 'app_AMT_GOODS_PRICE',  'app_OCCUPATION_TYPE_Laborers',  'app_AMT_CREDIT',  'app_AMT_ANNUITY']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8XMPsdYNwJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Estimate another logit model using one of the above variable subsets\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwRQQSadcW9M",
        "colab_type": "text"
      },
      "source": [
        "Success! Finally!\n",
        "Extract the interceot and coefficients your model for a quick check of the values look plausible. Then calculate the classification accuracy of your model using the training data. This is possible using a single line of code. Try finding the right function, which is part of scikit-learn. Also apply your model to the test data and calculate predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsPLK3rWNxQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to print model coefficient and calculate accuracy on the training set\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muMAKf0rnbgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate test set predictions and visualize their distribution\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-YTowBOXoPw",
        "colab_type": "text"
      },
      "source": [
        "### Variable importance\n",
        "\n",
        "Logistic regression has the nice benefit of giving us information how \"important\" a variable is.\n",
        "\n",
        "Plot the top ten most important variables. Do the variables which you identified in the EDA show up? If you find variables in the list, you haven't looked at in the EDA, consider going back to the EDA task to plot them to get an intuition on why they might be important.\n",
        "\n",
        "**Hint:** If you have used the statsmodel library for your logit model, the above task should be easy. If you developed the logit model using scikit-learn, you cannot easily get p-values for regression coefficients. See [this post for some details and remedies](https://datascience.stackexchange.com/questions/15398/how-to-get-p-value-and-confident-interval-in-logisticregression-with-sklearn). A crude but workable solution for scikit-learn users is to select important variables by looking at the magnitude of the estimated coefficients. Provided you did scale your data, which we did before, coefficient magnitudes signal variable importance. We will discuss more powerful and versatile ways to determine variable importance in the course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXHBacx-Yh-z",
        "colab_type": "code",
        "outputId": "7b9162a5-b7da-4bf7-f242-1d9a8c735f7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Code to visualize variable importance"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAEGCAYAAAC5CuvGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBzRvDqOzFl4hAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7i-FcjjkaG",
        "colab_type": "text"
      },
      "source": [
        "## The next steps: \n",
        "Given that this is a kaggle competition you can actually submit your prediction on the test dataset to the kaggle leaderboard and see how well you perform. Do not be too disappointed if not achieving a high place in the leaderboard ;)\n",
        "\n",
        "Feel free to further tweak your model by, for example, exploring other datasets of the competition, engineering new variables, etc. We will continue working with the data in the course and learn about various options to create a good  classification model."
      ]
    }
  ]
}