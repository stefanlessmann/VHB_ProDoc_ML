{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e3aace",
   "metadata": {},
   "source": [
    "<div style=\"position:relative; width:100%; height:200px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/stefanlessmann/VHB_ProDoc_ML/master/banner-nb.png\" style=\"width:100%; object-fit:cover;\" alt=\"ProDok-MachineLearning-Banner\">\n",
    "  <div style=\"\n",
    "      position:absolute;\n",
    "      left:4%;\n",
    "      top:50%;\n",
    "      transform:translateY(-50%);\n",
    "      font-size:3.2vw;\n",
    "      font-weight:750;\n",
    "      color:#1f2a44;\">\n",
    "    ProDok â€“ Machine Learning\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c17609",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stefanlessmann/VHB_ProDoc_ML/blob/master/P.I.1.data_exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090cc8d",
   "metadata": {},
   "source": [
    "\n",
    "# P.I.1. Data Exploration and Preparation Using Python\n",
    "The practice sessions complement the lectures and provide hands-on experience with the concepts covered in the course. This session focuses on explanatory data analysis (EDA), data preparation, and the subtle but crucial differences between explanatory and predictive modeling. \n",
    "\n",
    "The available time does not facilitate practicing Python programming. We will use generative AI (i.e., an LLM) to generate relevant codes and focus on designing effective prompts and discussing data science outputs emerging from the generated Python codes.\n",
    "\n",
    "<p class=\"alert alert-warning\"><strong>Disclaimer:</strong><br> It is crucial to carefully inspect generated codes to ensure their correctness and suitability for a task at hand. Never use generated codes without ample testing and verification in research or practice sessions. We will try to devote as much attention to this pivotal aspect as is possible in the available time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc38bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b1579",
   "metadata": {},
   "source": [
    "# Credit Risk Analytics Data \n",
    "We will work with a synthetic dataset that simulates credit risk analytics data. Specifically, the dataset represents credit applicants at the time of a loan application. It provides information about the applicants, their financial situation, loan characteristics, and so on, as detailed in the following [data dictionary](## Data Dictionary). \n",
    "\n",
    "The dataset includes two target variables to facilitate classifcation and regression modeling. A binary target indicates whether the applicant defaulted on the loan. In case of default, a numerical variable `LGD` gives the share of the outstanding amount that was lost (i.e., loss-given-default).\n",
    "\n",
    "All data are synthetic and created for educational purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://raw.githubusercontent.com/stefanlessmann/VHB_ProDoc_ML/master/credit_data_100k.csv\"\n",
    "\n",
    "# Run this line to load the data from GitHub\n",
    "#df = pd.read_csv(data_url)\n",
    "\n",
    "# If you stored the data locally, use this line instead (faster!)\n",
    "df = pd.read_csv(\"credit_data_100k.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8800ae5",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    "---\n",
    "\n",
    "### Outcome Variables\n",
    "\n",
    "| Variable | Type | Meaning |\n",
    "|----------|------|--------|\n",
    "| `default_12m` | binary (0/1) | Indicates whether the loan defaulted within 12 months after origination (1 = default, 0 = no default). |\n",
    "| `LGD` | numeric | Loss Given Default (fraction of exposure lost if a default occurs). Defined only for loans that defaulted; otherwise missing. |\n",
    "\n",
    "---\n",
    "\n",
    "### Applicant Information\n",
    "\n",
    "| Variable | Type | Meaning |\n",
    "|----------|------|--------|\n",
    "| `age` | integer | Applicant age in years at application time. |\n",
    "| `income` | numeric (â‚¬) | Applicant annual income. |\n",
    "| `employment_type` | categorical | Employment category (`salaried`, `self_employed`, `public_sector`, `student`, `retired`). |\n",
    "| `housing_status` | categorical | Housing situation (`rent`, `own`, `mortgage`, `family`). |\n",
    "| `years_at_job` | numeric | Years in current job. |\n",
    "| `years_at_address` | numeric | Years at current residence. |\n",
    "| `region` | categorical | Geographic region (`north`, `south`, `east`, `west`, `metro`). |\n",
    "\n",
    "---\n",
    "\n",
    "### Relationship with the Bank\n",
    "\n",
    "| Variable | Type | Meaning |\n",
    "|----------|------|--------|\n",
    "| `has_bank_account` | binary (0/1) | Indicates whether the applicant already has an account with the bank (1 = yes, 0 = no). |\n",
    "\n",
    "---\n",
    "\n",
    "### Loan Characteristics\n",
    "\n",
    "| Variable | Type | Meaning |\n",
    "|----------|------|--------|\n",
    "| `loan_purpose` | categorical | Stated purpose of the loan (`debt_consolidation`, `car`, `home_improvement`, `education`, `other`). |\n",
    "| `loan_amount` | numeric (â‚¬) | Requested loan amount. |\n",
    "| `secured` | binary (0/1) | Indicates whether the loan is secured by collateral (1 = secured, 0 = unsecured). |\n",
    "| `dti` | numeric | Debt-to-income ratio (total monthly debt payments divided by monthly income). |\n",
    "\n",
    "---\n",
    "\n",
    "### Credit-History-Related Indicators\n",
    "\n",
    "| Variable | Type | Meaning |\n",
    "|----------|------|--------|\n",
    "| `num_inquiries_6m` | integer | Number of credit inquiries in the past 6 months. |\n",
    "| `num_delinquencies_24m` | integer | Number of delinquencies recorded in the past 24 months. |\n",
    "| `num_collections` | integer | Number of accounts currently in collection. |\n",
    "| `card_utilization` | numeric | Utilization ratio on the bankâ€™s credit card (balance divided by credit limit). May be missing if not available. |\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Missing values are represented as `NaN`.\n",
    "- All variables are synthetic and do not correspond to real individuals or institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c729e",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis (EDA)\n",
    "Explanatory data analysis (EDA) aims at gaining first-level insight into a dataset, and is the first step in empirical research/when beginning to work with a novel dataset.\n",
    "\n",
    "The goal of this exercise is to use Python for EDA. We will *LLM-generate* the corresponding codes. Below we provide a possible prompt template, which we will discuss and complete in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164de57",
   "metadata": {},
   "source": [
    "## EDA Prompt Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ddda9dd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Scenario:\n",
    "---\n",
    "You are a credit risk analyst working for a major retail bank. \n",
    "You received a historical dataset for analysis. The data dictionary \n",
    "is given below in triple quotes.\n",
    "```data dictionary\n",
    "\n",
    "COPY DATA DICTIONARY HERE\n",
    "\n",
    "```\n",
    "The data is new to you. Your aim to gain preliminary insights by \n",
    "performing explanatory data analysis (EDA).\n",
    "\n",
    "Tasks:\n",
    "---\n",
    "Generate Python code to perform EDA. Your EDA should include the following steps:\n",
    "- ...\n",
    "- TO BE DISCUSSED IN THE SESSION \n",
    "- ...\n",
    "\n",
    "Guidelines:\n",
    "---\n",
    "- Import relevant libraries to ensure your code is self-contained.\n",
    "- Organize the code into logical sections with headings for clarity.\n",
    "- Assume the data is already available in a DataFrame called `df`. Do not modify this variable. If transformations are required, create a working copy named df_eda.\n",
    "- Include comments explaining each step of the analysis.\n",
    "- Display all results clearly and label plots with meaningful titles and axis labels.\n",
    "- Prefer a small number of informative plots over many repetitive ones.\n",
    "- Use the data dictionary to guide which columns are numeric/categorical/identifiers.\n",
    "- Avoid computationally expensive plots. Only suggest analysis steps that you consider informative but would involve heavy computation.\n",
    "- End with a short bullet-point summary of key findings (max 8 bullets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850271e7",
   "metadata": {},
   "source": [
    "## LLM-Generated Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your generated codes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f06970d",
   "metadata": {},
   "source": [
    "# Data Preparation (DPP)\n",
    "The goal of the data preparation is to address potential data quality issue (e.g., identified during EDA) and to facilitate statistical / machine learning modeling. For example, many statistical methods cannot handle missing values. Linear regression is a popular example. Thus, addressing missing values is a standard dpp task.\n",
    "\n",
    "We proceed as before: providing a template of a prompt to generate dpp code, which we will discuss and complete in class. \n",
    " \n",
    ">Remark: We assume you execute the dpp prompt in the same chat session as the previous EDA prompt. Doing so ensures that the LLM has stored information on the dataset and the results of the EDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fd924",
   "metadata": {},
   "source": [
    "## DPP Prompt Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd9d00ff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Scenario:\n",
    "---\n",
    "You previously performed EDA on the loan application dataset provided earlier in this session. Based on those insights, you now prepare the data for subsequent statistical or machine learning modeling.\n",
    "Next, you aim to clean and prepare the dataset. The result of this step\n",
    "should be a modeling-ready version of the data.\n",
    "\n",
    "Tasks:\n",
    "---\n",
    "Generate Python code to perform data preparation. Preparation actions should be \n",
    "guided by the EDA findings and typical requirements of regression models.\n",
    "\n",
    "Your preparation should include:\n",
    "\n",
    "- ...\n",
    "- TO BE DISCUSSED IN THE SESSION\n",
    "- ...\n",
    "\n",
    "Guidelines:\n",
    "---\n",
    "- Import relevant libraries so the code is self-contained.\n",
    "- The dataset is available as a pandas DataFrame named `df`. Do not modify the original DataFrame df. Create a working copy named `df_prep`.\n",
    "- Annotate the code with comments explaining the rationale behind each preparation step. \n",
    "- Clearly separate preparation steps into logical sections with headings.\n",
    "- Ensure all transformations are reproducible and applied programmatically.\n",
    "- Use the data dictionary and EDA findings from earlier in this session to guide preparation choices.\n",
    "- Prefer simple, interpretable preparation over complex transformations.\n",
    "- Where appropriate, include brief comments explaining the rationale of each step.\n",
    "- Display key intermediate checks (e.g., shapes, missingness after preparation, variable groups).\n",
    "- At the end, produce a final modeling dataset named df_model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e310d1",
   "metadata": {},
   "source": [
    "## LLM-Generated result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b725141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy your generated dpp codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207c798",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "Linear regression is perhaps the most widely used statistical method in empirical research. A key learning goal of this exercise is to stress the subtle differences between an *explanatory* and a *predictive* model. Linear regression supports both, explanatory and predictive modeling. Traditionally, using linear regression for explanatory modeling is more common.   \n",
    "\n",
    "Below, we provide a working example of running linear regression in Python using the `statsmodels` library. Unlike the famous `sklearn` library, the *go to library* for machine learning in Python, the `statsmodels` library focuses on explanatory modeling. \n",
    "\n",
    "The code assume a prepared *ready-to-use* version of the dataset is stored in a variable `df_model`. As dependent variable, we use `lgd`, which is defined only for defaulted loans. Therefore, the code includes a filtering step to select only the relevant subset of the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c07f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# ---------------------------\n",
    "# 1) DEFINE TARGET (y)\n",
    "# ---------------------------\n",
    "# LGD is defined only for defaulted loans â†’ restrict to observed LGD\n",
    "if \"lgd\" not in df_model.columns:\n",
    "    raise ValueError(\"LGD not found in df_model.\")\n",
    "\n",
    "df_lgd = df_model[df_model[\"lgd\"].notna()].copy()\n",
    "print(\"Rows with observed LGD:\", df_lgd.shape[0])\n",
    "\n",
    "y = df_lgd[\"lgd\"].astype(np.float64)  # ensure numeric target\n",
    "\n",
    "# ---------------------------\n",
    "# 2) DEFINE PREDICTORS (X)\n",
    "# ---------------------------\n",
    "# Exclude:\n",
    "# - LGD (target)\n",
    "# - default_12m (post-event indicator / not a valid predictor for LGD)\n",
    "# - any known leakage/reference variables (e.g., default_12m)\n",
    "exclude_cols = [\"lgd\", \"default_12m\"]\n",
    "X_cols = [c for c in df_lgd.columns if c not in exclude_cols]\n",
    "X = df_lgd[X_cols].copy()\n",
    "X = X.astype(np.float64)  # needed for statsmodels: ensure pandas / numpy data type compatibility \n",
    "print(\"Number of predictors before checks:\", X.shape[1])\n",
    "\n",
    "# Ensure all predictors are numeric\n",
    "non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(\"Dropping non-numeric predictors:\", non_numeric)\n",
    "    X = X.drop(columns=non_numeric)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) ADD INTERCEPT\n",
    "# ---------------------------\n",
    "X = sm.add_constant(X, has_constant=\"add\")\n",
    "\n",
    "print(\"Design matrix shape (with intercept):\", X.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) FIT LINEAR REGRESSION\n",
    "# ---------------------------\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# ---------------------------\n",
    "# 5) OUTPUT RESULTS\n",
    "# ---------------------------\n",
    "print(\"\\n=== OLS SUMMARY ===\")\n",
    "print(results.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce60e4a",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "\n",
    "### Residuals vs. Fitted Values\n",
    "Plotting residuals against fitted values is a common diagnostic tool to assess the assumptions of linear regression, such as homoscedasticity (constant variance of residuals) and linearity. The plot should ideally show a random scatter of points around the horizontal line at zero, without any discernible pattern. A funnel shape (i.e., increasing or decreasing spread of residuals) may indicate heteroscedasticity, while a systematic pattern (e.g., a curve) may suggest nonlinearity.\n",
    "\n",
    "It is common practice to standardize residuals before plotting. This helps to identify outliers and to better assess the distribution of residuals. \n",
    "\n",
    "Other popular visualizations for residual analysis include the *Q-Q Plot*: A quantile-quantile plot compares the distribution of residuals to a theoretical distribution (e.g., normal distribution) to assess normality. If the residuals are normally distributed, the points in the Q-Q plot will approximately lie on a straight line. Deviations from this line may indicate non-normality, which can affect inference in linear regression.\n",
    "\n",
    "Below we provide code to create these visualizations using the `seaborn` library for the residuals vs. fitted values plot and the `statsmodels` library for the Q-Q plot. \n",
    "\n",
    "We will briefly discuss the results in class. In general, the plots confirm what the $R^2$ value already suggested: The linear regression model does not fit the data well and is an inappropriate choice for this dataset. This was to be expected in an *LGD model* ðŸ˜‰.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals and fitted values\n",
    "yhat = results.fittedvalues\n",
    "e = results.resid\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1.1 Residuals vs fitted (seaborn preferred over matplotlib)\n",
    "sns.scatterplot(x=yhat, y=e, ax=axes[0, 0], alpha=0.4)\n",
    "axes[0, 0].axhline(0)\n",
    "axes[0, 0].set_title(\"Residuals vs Fitted\")\n",
    "axes[0, 0].set_xlabel(\"Fitted values\")\n",
    "axes[0, 0].set_ylabel(\"Residuals\")\n",
    "\n",
    "# 1.2 Standardized residuals vs fitted (statsmodels influence)\n",
    "# compute standardized residuals from e\n",
    "e_std = (e - np.mean(e)) / np.std(e)\n",
    "\n",
    "sns.scatterplot(x=yhat, y=e_std, ax=axes[0, 1], alpha=0.4)\n",
    "axes[0, 1].axhline(0)\n",
    "axes[0, 1].axhline(2, linestyle=\"--\")\n",
    "axes[0, 1].axhline(-2, linestyle=\"--\")\n",
    "axes[0, 1].set_title(\"Standardized Residuals vs Fitted\")\n",
    "axes[0, 1].set_xlabel(\"Fitted values\")\n",
    "axes[0, 1].set_ylabel(\"Standardized residuals\")\n",
    "\n",
    "# 2.1 Histogram (seaborn)\n",
    "sns.histplot(e, bins=40, ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Histogram of residuals\")\n",
    "axes[1, 0].set_xlabel(\"Residual\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "\n",
    "# 2.2 Q-Q plot (statsmodels preferred)\n",
    "sm.qqplot(e, line=\"45\", ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"Q-Q plot of residuals\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8596a02",
   "metadata": {},
   "source": [
    "# Conclusions and Outlook\n",
    "The regression statistics suggest that the independent variable explain variation in LGD only to a very limited degree (e.g., $ R^2 < 0.1$). Therefore, we would not expect to a *predictive* model, which uses the same variables to forecast unknown LGDs accurately. You may wonder whether this is a *fault* of linear regression and whether advanced ML models would perform better. This points warrants a discussion in class. \n",
    "\n",
    "In the next coding session, we will consider such advanced ML approaches, although in a different use case: binary classification for credit default prediction. After completing this session, you will be equipped with several coding examples illustrating standard ML practices. You are welcome to then come back to this notebook and examine, empirically, if advanced ML beats linear regression in this LGD modeling use case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bads310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
